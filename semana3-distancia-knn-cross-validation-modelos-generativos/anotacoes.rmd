---
title: "Distancia"
output: html_document
---
```{r}
library(tidyverse)
library(dslabs)
library(ggplot2)
```
```{r}
mnist <- read_mnist()
set.seed(0, sample.kind = "Rounding")
ind <- which(mnist$train$labels %in% c(2,7)) %>% sample(500)
x <- mnist$train$images[ind,]
y <- mnist$train$labels[ind]
```
```{r}
d <- dist(x)
class(d)
as.matrix(d)[1:3,1:3]
```
```{r}
image(as.matrix(d))
```
```{r}
image(as.matrix(d)[order(y), order(y)])
```
# KNN (k-Nearest-Neighbors
O primeiro modelo de machine learning que será apresentado. Consiste basicamente de uma suavização por caixa em
 multiplas dimensões. Para podermos fazer isso, precisamos primeiramente definir a distancia baseado nos preditores, e
então calcular a média dos k pontos mais próximos. O conjunto de pontos usados para isso se chama neighborhood.
Dessa forma, temos uma estimativa das probabilidades condicionais.

Para validar o modelo, vamos compara-lo com a regressão logística
```{r}
library(tidyverse)
library(dslabs)
data("mnist_27")
mnist_27$test %>% ggplot(aes(x_1, x_2, color = y)) + geom_point()
```
```{r}
library(caret)
fit_glm <- glm(y~x_1+x_2, data=mnist_27$train, family="binomial")
p_hat_logistic <- predict(fit_glm, mnist_27$test)
y_hat_logistic <- factor(ifelse(p_hat_logistic > 0.5, 7, 2))
confusionMatrix(data = y_hat_logistic, reference = mnist_27$test$y)
```
Temos uma precisão de 76%. Enquanto em um modelo por KNN:
```{r}
knn_fit <- knn3(y ~ ., data = mnist_27$train, k=5)

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)
```
Como o dataset é balanceado, podemos considerar apenas a precisão como medida, uma vez que tanto sensitividade e
sensibilidade são igualmente importantes. Vemos que o KNN teve, de fato uma precisão maior. Podemos comparar os modelos
visualmente:
```{r}
mnist_27$true_p %>% ggplot(aes(x_1, x_2, z = p, fill = p)) +
        geom_raster() +
        scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
        stat_contour(breaks=c(0.5), color="black")

mnist_27$true_p %>% mutate(knn=predict(knn_fit, mnist_27$true_p)[, 2]) %>% ggplot(aes(x_1, x_2, z = knn, fill = knn)) +
        geom_raster() +
        scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
        stat_contour(breaks=c(0.5), color="black")

mnist_27$true_p %>% mutate(rl=predict(fit_glm, mnist_27$true_p)) %>% ggplot(aes(x_1, x_2, z = rl, fill = rl)) +
        geom_raster() +
        scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
        stat_contour(breaks=c(0.5), color="black")
```
Vemos que dos dois modelos, o que mais se aproximou da curva de probabilidade real foi o KNN. Ainda assim, vemos a
existencia de "bolhas" azuis onde a probabilidade deveria apontar ser um 2 e vice-versa. Isso ocorre por problemas de
over fitting, quando o modelo se adequa tão bem ao dado de teste, que ignora a tendencia dos dados.

Para corrigir isso, podemos testar diferentes valores de k para mudar a variação de cada estimativa:
```{r}
ks <- seq(3, 251, 2)
library(purrr)
accuracy <- map_df(ks, function(k){
  fit <- knn3(y ~ ., data = mnist_27$train, k = k)
  y_hat <- predict(fit, mnist_27$train, type = "class")
  cm_train <- confusionMatrix(data = y_hat, reference = mnist_27$train$y)
  train_error <- cm_train$overall["Accuracy"]
  y_hat <- predict(fit, mnist_27$test, type = "class")
  cm_test <- confusionMatrix(data = y_hat, reference = mnist_27$test$y)
  test_error <- cm_test$overall["Accuracy"]
  tibble(train = train_error, test = test_error)
})

#pick the k that maximizes accuracy using the estimates built on the test data
ks[which.max(accuracy$test)]
max(accuracy$test)
```
Vemos que com k=41 a precisão do modelo é de 86% consideravelmente maior que os 81% do modelo com k=5. Se plotarmos
o modelo, temos:
```{r}
fit <- knn3(y ~ ., data = mnist_27$train, k = 41)
mnist_27$true_p %>% mutate(knn=predict(fit, mnist_27$true_p)[, 2]) %>% ggplot(aes(x_1, x_2, z = knn, fill = knn)) +
        geom_raster() +
        scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
        stat_contour(breaks=c(0.5), color="black")
```
E vemos que esse modelo é realmente bem mais próximo da probabilidade real. Entretanto, ao fazer isso notamos que usamos
o dataset de treino para selecionar o melhor k, o que não deveria acontecer, ja que dessa forma não temos um dataset
em que o modelo não foi treinado para validação.